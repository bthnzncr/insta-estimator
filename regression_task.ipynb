{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from google.colab import drive\n",
        "import xgboost as xgb\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================================\n",
        "# STEP 0: Define file paths and initialize dictionaries\n",
        "# ================================\n",
        "train_data_path = \"/content/drive/MyDrive/released_dataset/training-dataset.jsonl.gz\"\n",
        "test_data_path  = \"/content/drive/MyDrive/released_dataset/test-regression-round3.jsonl\"\n",
        "\n",
        "username2posts_train = {}\n",
        "username2profile_train = {}\n",
        "\n",
        "# ================================\n",
        "# STEP 1: Load Data\n",
        "# ================================\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile[\"username\"]\n",
        "        username2posts_train[username] = sample[\"posts\"]\n",
        "        username2profile_train[username] = profile\n",
        "\n",
        "print(\"Finished loading training dataset.\")\n",
        "\n",
        "# --- Define helper function for word count ---\n",
        "def word_count(caption):\n",
        "    return len(caption.split())\n",
        "\n",
        "# ================================\n",
        "# STEP 2: Calculate average like_count, comment_count and word count per user\n",
        "# ================================\n",
        "user2avg_likes_train = {}\n",
        "user2avg_comments_train = {}\n",
        "user2avg_wordcount_train = {}\n",
        "for uname, posts in username2posts_train.items():\n",
        "    total_likes = 0.0\n",
        "    total_comments = 0.0\n",
        "    count_posts = 0\n",
        "    total_word_count = 0.0\n",
        "    for post in posts:\n",
        "        caption = post.get(\"caption\", \"\") or \"\"\n",
        "        # Get like and comment counts; if negative, set to 0.\n",
        "        like_count = post.get(\"like_count\", 0) or 0\n",
        "        if like_count < 0:\n",
        "            like_count = 0.0\n",
        "        comments_count = post.get(\"comments_count\", 0) or 0\n",
        "        total_likes += like_count\n",
        "        total_comments += comments_count\n",
        "        count_posts += 1\n",
        "        caption_word_count = word_count(caption)\n",
        "        total_word_count += caption_word_count\n",
        "\n",
        "    user2avg_likes_train[uname] = total_likes / count_posts if count_posts > 0 else 0.0\n",
        "    user2avg_wordcount_train[uname] = total_word_count / count_posts if count_posts > 0 else 0.0\n",
        "    user2avg_comments_train[uname] = total_comments / count_posts if count_posts > 0 else 0.0\n",
        "\n",
        "global_avg_likes = float(np.mean(list(user2avg_likes_train.values()))) if user2avg_likes_train else 0.0\n",
        "global_avg_comment = float(np.mean(list(user2avg_comments_train.values()))) if user2avg_comments_train else 0.0\n",
        "global_avg_word_count = float(np.mean(list(user2avg_wordcount_train.values()))) if user2avg_wordcount_train else 0.0\n",
        "\n",
        "# ================================\n",
        "# Helper Functions for Feature Extraction\n",
        "# ================================\n",
        "def count_hashtags(caption):\n",
        "    return len(re.findall(r\"#\\w+\", caption))\n",
        "\n",
        "def count_mentions(caption):\n",
        "    return len(re.findall(r\"@\\w+\", caption))\n",
        "\n",
        "def count_emojis(caption):\n",
        "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F]\", flags=re.UNICODE)\n",
        "    return len(emoji_pattern.findall(caption))\n",
        "\n",
        "def count_urls(caption):\n",
        "    return len(re.findall(r\"http[s]?://\\S+\", caption))\n",
        "\n",
        "def punctuation_count(caption):\n",
        "    return len(re.findall(r\"[!?.]\", caption))\n",
        "\n",
        "def text_length(caption):\n",
        "    return len(caption)\n",
        "\n",
        "def word_count(caption):\n",
        "    return len(caption.split())\n",
        "\n",
        "# ================================\n",
        "# STEP 3: Build the post-level training rows with extra features\n",
        "# ================================\n",
        "train_rows = []\n",
        "\n",
        "for uname, posts in username2posts_train.items():\n",
        "    profile = username2profile_train.get(uname, {})\n",
        "    follower_count  = profile.get(\"follower_count\", 0) or 0\n",
        "    following_count = profile.get(\"following_count\", 0) or 0\n",
        "    user_avg_likes = user2avg_likes_train.get(uname, global_avg_likes)\n",
        "    user_avg_comments = user2avg_comments_train.get(uname, global_avg_comment)\n",
        "    user_avg_word_count = user2avg_wordcount_train.get(uname, global_avg_word_count)\n",
        "    # Boolean features (casting to int)\n",
        "    is_business = int(profile.get(\"is_business_account\", False))\n",
        "    is_supervision_enabled = int(profile.get(\"is_supervision_enabled\", False))\n",
        "    is_verified = int(profile.get(\"is_verified\", False))\n",
        "    is_professional_account = int(profile.get(\"is_professional_account\", False))\n",
        "\n",
        "    for post in posts:\n",
        "        # Get like_count and reset negative values to 0.\n",
        "        like_count = post.get(\"like_count\", 0) or 0\n",
        "        if like_count < 0:\n",
        "            like_count = 0.0\n",
        "\n",
        "        if like_count == 0:\n",
        "            like_count = 0.01\n",
        "\n",
        "        comments_count = post.get(\"comments_count\", 0) or 0\n",
        "        pid = post.get(\"id\", 0) or 0  # unique ID\n",
        "        caption = post.get(\"caption\", \"\") or \"\"\n",
        "\n",
        "        # Timestamp\n",
        "        timestamp_str = post.get(\"timestamp\", None)\n",
        "        post_hour = np.nan\n",
        "        post_dayofweek = np.nan\n",
        "        post_month = np.nan\n",
        "        timestamp = None\n",
        "        if timestamp_str:\n",
        "            try:\n",
        "\n",
        "                timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "                post_hour = timestamp.hour\n",
        "                post_dayofweek = timestamp.weekday()\n",
        "                post_month = timestamp.month\n",
        "            except Exception as e:\n",
        "\n",
        "                post_hour = np.nan\n",
        "                post_dayofweek = np.nan\n",
        "                post_month = np.nan\n",
        "\n",
        "\n",
        "        num_hashtags = count_hashtags(caption)\n",
        "        num_mentions = count_mentions(caption)\n",
        "        num_emojis = count_emojis(caption)\n",
        "        num_urls = count_urls(caption)\n",
        "        num_punctuations = punctuation_count(caption)\n",
        "        caption_length = text_length(caption)\n",
        "        caption_word_count = word_count(caption)\n",
        "\n",
        "\n",
        "        comment_diff = user_avg_comments - comments_count\n",
        "\n",
        "\n",
        "        if user_avg_comments > 0:\n",
        "            r = comments_count / user_avg_comments\n",
        "        else:\n",
        "            r = 0.0\n",
        "        norm_comment_ratio = (r - 1) / (r + 1)\n",
        "\n",
        "\n",
        "        follower_business = follower_count * is_business\n",
        "\n",
        "        row_dict = {\n",
        "            \"id\": pid,\n",
        "            \"username\": uname,\n",
        "            \"like_count\": like_count,\n",
        "            \"user_avg_likes\": user_avg_likes,\n",
        "            \"user_avg_comments\": user_avg_comments,\n",
        "            \"comment_count\": comments_count,\n",
        "            \"comment_diff\": comment_diff,\n",
        "            \"norm_comment_ratio\": norm_comment_ratio,\n",
        "            \"following_count\": following_count,\n",
        "            \"follower_count\": follower_count,\n",
        "            \"follower_business\": follower_business,\n",
        "            \"is_business\": is_business,\n",
        "            \"is_verified\": is_verified,\n",
        "            \"is_professional_account\": is_professional_account,\n",
        "            \"is_supervision_enabled\": is_supervision_enabled,\n",
        "            \"num_hashtags\": num_hashtags,\n",
        "            \"num_mentions\": num_mentions,\n",
        "            \"num_emojis\": num_emojis,\n",
        "            \"num_urls\": num_urls,\n",
        "            \"num_punctuations\": num_punctuations,\n",
        "            \"caption_length\": caption_length,\n",
        "            \"caption_word_count\": caption_word_count,\n",
        "            \"user_avg_word_count\": user_avg_word_count,\n",
        "\n",
        "            \"post_hour\": post_hour,\n",
        "            \"post_dayofweek\": post_dayofweek,\n",
        "            \"post_month\": post_month\n",
        "\n",
        "        }\n",
        "\n",
        "        train_rows.append(row_dict)\n",
        "\n",
        "# Optionally, save train_rows to a JSON file for debugging\n",
        "output_json_path = \"train_rows.json\"\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(train_rows, json_file, indent=4, default=str)\n",
        "print(f\"train_rows saved to {output_json_path}.\")\n",
        "\n",
        "# ================================\n",
        "# STEP 4: Load DataFrame and Prepare Features/Target\n",
        "# ================================\n",
        "df_reg_train = pd.DataFrame(train_rows)\n",
        "print(\"Post-level train DataFrame shape:\", df_reg_train.shape)\n",
        "print(df_reg_train.head(5))\n",
        "\n",
        "\n",
        "feature_cols = [\n",
        "    \"user_avg_likes\",\n",
        "    \"following_count\",\n",
        "    \"follower_count\",\n",
        "\n",
        "    \"num_hashtags\",\n",
        "    \"is_business\",\n",
        "    \"is_verified\",\n",
        "    \"user_avg_comments\",\n",
        "    \"comment_count\",\n",
        "    \"norm_comment_ratio\",\n",
        "\n",
        "    \"post_dayofweek\",\n",
        "\n",
        "]\n",
        "target_col = \"like_count\"\n",
        "\n",
        "X_all = df_reg_train[feature_cols].values\n",
        "y_all = df_reg_train[target_col].values\n",
        "\n",
        "# ================================\n",
        "# STEP 5: Split Overall Data into Training and Validation Sets\n",
        "# (Select exactly 3000 random samples for the validation set)\n",
        "# ================================\n",
        "np.random.seed(42)  # for reproducibility\n",
        "total_samples = df_reg_train.shape[0]\n",
        "validation_size = 3000\n",
        "\n",
        "all_indices = np.arange(total_samples)\n",
        "val_indices = np.random.choice(all_indices, size=validation_size, replace=False)\n",
        "train_indices = np.setdiff1d(all_indices, val_indices)\n",
        "\n",
        "df_train = df_reg_train.iloc[train_indices].copy()\n",
        "df_val   = df_reg_train.iloc[val_indices].copy()\n",
        "\n",
        "print(f\"Overall Training set: {df_train.shape[0]} samples\")\n",
        "print(f\"Overall Validation set: {df_val.shape[0]} samples (should be exactly 3000)\")\n",
        "\n",
        "# Reset index of validation set to ensure it is 0 to 2999\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "\n",
        "# ================================\n",
        "# STEP 6: Normalize the Data and Train the Model\n",
        "# ================================\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = df_train[feature_cols].values.astype(np.float32)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "X_val = df_val[feature_cols].values.astype(np.float32)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "y_train = df_train[target_col].values.astype(np.float32)\n",
        "\n",
        "model = RandomForestRegressor(random_state=42, n_estimators = 50, max_depth = 100)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# STEP 7: Evaluate on the Validation Set and Adjust Predictions\n",
        "# ================================\n",
        "y_val_pred = model.predict(X_val_scaled)\n",
        "adjusted_preds = []\n",
        "for i, row in df_val.iterrows():\n",
        "    # Custom post-prediction logic can be applied here if needed.\n",
        "    pred = y_val_pred[i]\n",
        "    adjusted_preds.append(pred)\n",
        "\n",
        "validation_results = pd.DataFrame({\n",
        "    'Actual': df_val[target_col],\n",
        "    'Predicted': adjusted_preds\n",
        "})\n",
        "print(\"\\n=== Actual vs Predicted Like Counts (first 100 rows) ===\")\n",
        "print(validation_results.head(100))\n",
        "\n",
        "epsilon = 1e-10\n",
        "individual_errors = np.log10(np.abs(df_val[target_col] - np.array(adjusted_preds)) + epsilon)\n",
        "sum_log_errors = np.sum(individual_errors)\n",
        "print(f\"\\nSum of log10(individual absolute errors) = {sum_log_errors:.4f}\")\n",
        "\n",
        "# ================================\n",
        "# STEP 8: Save the Predictions to CSV\n",
        "# ================================\n",
        "output_csv_path = \"predicted_like_counts_validation.csv\"\n",
        "df_val[\"predicted_like_count\"] = adjusted_preds\n",
        "df_val.to_csv(output_csv_path, index=False)\n",
        "print(f\"Validation predictions saved to {output_csv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg-GszGf4PKv",
        "outputId": "d88b3617-578a-4905-b448-c46d77546a08"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Finished loading training dataset.\n",
            "train_rows saved to train_rows.json.\n",
            "Post-level train DataFrame shape: (187302, 26)\n",
            "                  id    username  like_count  user_avg_likes  \\\n",
            "0  17990918969458720  deparmedya         6.0       11.542857   \n",
            "1  18219250732221045  deparmedya        22.0       11.542857   \n",
            "2  18311380465102328  deparmedya        19.0       11.542857   \n",
            "3  18089518138361507  deparmedya        19.0       11.542857   \n",
            "4  18012743929758497  deparmedya        21.0       11.542857   \n",
            "\n",
            "   user_avg_comments  comment_count  comment_diff  norm_comment_ratio  \\\n",
            "0           0.342857              0      0.342857           -1.000000   \n",
            "1           0.342857              1     -0.657143            0.489362   \n",
            "2           0.342857              0      0.342857           -1.000000   \n",
            "3           0.342857              1     -0.657143            0.489362   \n",
            "4           0.342857              0      0.342857           -1.000000   \n",
            "\n",
            "   following_count  follower_count  ...  num_mentions  num_emojis  num_urls  \\\n",
            "0              192            1167  ...             0           0         0   \n",
            "1              192            1167  ...             0           0         0   \n",
            "2              192            1167  ...             0           0         0   \n",
            "3              192            1167  ...             0           0         0   \n",
            "4              192            1167  ...             0           0         0   \n",
            "\n",
            "   num_punctuations  caption_length  caption_word_count  user_avg_word_count  \\\n",
            "0                 1              41                   4            13.085714   \n",
            "1                 0              76                   5            13.085714   \n",
            "2                 0              30                   2            13.085714   \n",
            "3                 0              58                   3            13.085714   \n",
            "4                 0              39                   4            13.085714   \n",
            "\n",
            "   post_hour  post_dayofweek  post_month  \n",
            "0          9               6          10  \n",
            "1         19               1           8  \n",
            "2         21               0           8  \n",
            "3         21               0           8  \n",
            "4         21               0           8  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "Overall Training set: 184302 samples\n",
            "Overall Validation set: 3000 samples (should be exactly 3000)\n",
            "\n",
            "=== Actual vs Predicted Like Counts (first 100 rows) ===\n",
            "     Actual     Predicted\n",
            "0      60.0     76.816000\n",
            "1    3230.0   6216.940000\n",
            "2     216.0    190.121970\n",
            "3      33.0     56.828667\n",
            "4      27.0     18.017071\n",
            "..      ...           ...\n",
            "95     33.0     59.880000\n",
            "96   1302.0    682.700000\n",
            "97  18082.0  18240.010000\n",
            "98    258.0    295.196667\n",
            "99     37.0     31.575238\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "\n",
            "Sum of log10(individual absolute errors) = 4431.4705\n",
            "Validation predictions saved to predicted_like_counts_validation.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QOWGDOLN4O6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRetraining on the entire dataset for final inference...\")\n",
        "final_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "final_model.fit(X_all, y_all)\n",
        "\n",
        "# ================================\n",
        "# STEP 10: Generate Predictions on Official Test Set Using the Same Feature Extraction\n",
        "# ================================\n",
        "print(\"\\n=== STEP 10: Generate Predictions on Official Test Set ===\")\n",
        "\n",
        "# 1) Load the test data\n",
        "official_test_posts = []\n",
        "with open(test_data_path, \"r\", encoding=\"utf-8\") as test_f:\n",
        "    for line in test_f:\n",
        "        official_test_posts.append(json.loads(line))\n",
        "\n",
        "output_improved_path = \"prediction-regression-improved.json\"\n",
        "predictions_improved = {}\n",
        "\n",
        "\n",
        "for sample in official_test_posts:\n",
        "    post_id = sample.get(\"id\", \"\")\n",
        "    username = sample.get(\"username\", \"\")\n",
        "\n",
        "\n",
        "    profile = username2profile_train.get(username, {})\n",
        "\n",
        "    user_avg_likes    = user2avg_likes_train.get(username, global_avg_likes)\n",
        "    user_avg_comments = user2avg_comments_train.get(username, global_avg_comment)\n",
        "    following_count   = profile.get(\"following_count\", 0) or 0\n",
        "    follower_count    = profile.get(\"follower_count\", 0) or 0\n",
        "    is_business       = int(profile.get(\"is_business_account\", False))\n",
        "    is_verified       = int(profile.get(\"is_verified\", False))\n",
        "\n",
        "\n",
        "    # Post-level data\n",
        "    caption = sample.get(\"caption\", \"\") or \"\"\n",
        "    comments_count = sample.get(\"comments_count\", 0) or 0\n",
        "\n",
        "    # Extract text feature: number of hashtags\n",
        "    num_hashtags = count_hashtags(caption)\n",
        "\n",
        "    # Compute normalized comment ratio following same logic as training:\n",
        "    if user_avg_comments > 0:\n",
        "        r = comments_count / user_avg_comments\n",
        "    else:\n",
        "        r = 0.0\n",
        "    norm_comment_ratio = (r - 1) / (r + 1)\n",
        "\n",
        "    # Extract timestamp-derived feature \"post_dayofweek\"\n",
        "    timestamp_str = sample.get(\"timestamp\", None)\n",
        "    post_dayofweek = np.nan\n",
        "    if timestamp_str:\n",
        "        try:\n",
        "            timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "            post_dayofweek = timestamp.weekday()  # Monday=0, Sunday=6\n",
        "        except Exception as e:\n",
        "            post_dayofweek = np.nan\n",
        "\n",
        "    # Build feature vector\n",
        "    row_feat = [\n",
        "        user_avg_likes,\n",
        "        following_count,\n",
        "        follower_count,\n",
        "        num_hashtags,\n",
        "        is_business,\n",
        "        is_verified,\n",
        "        user_avg_comments,\n",
        "        comments_count,\n",
        "        norm_comment_ratio,\n",
        "        post_dayofweek\n",
        "    ]\n",
        "\n",
        "\n",
        "    pred_like = final_model.predict([row_feat])[0]\n",
        "    pred_like = max(int(pred_like), 0)\n",
        "    predictions_improved[post_id] = pred_like\n",
        "\n",
        "\n",
        "with open(output_improved_path, \"w\", encoding='utf-8') as of:\n",
        "    json.dump(predictions_improved, of, indent=4)\n",
        "\n",
        "print(f\"Saved improved predictions to: {output_improved_path}\")\n",
        "print(\"=== End of Pipeline ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR6nvKncOIqA",
        "outputId": "49b68eae-f051-48a0-8aff-cd35853138f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retraining on the entire dataset for final inference...\n",
            "\n",
            "=== STEP 10: Generate Predictions on Official Test Set ===\n",
            "Saved improved predictions to: prediction-regression-improved.json\n",
            "=== End of Pipeline ===\n"
          ]
        }
      ]
    }
  ]
}